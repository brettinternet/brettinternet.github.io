{"version":"https://jsonfeed.org/version/1","title":"brett.cloud","home_page_url":"https://brett.cloud/","feed_url":"https://brett.cloud/feed.json","author":{"name":"Brett Gardiner","email":"hi@[domain]"},"items":[{"id":"e314906a-6b52-52b2-c989-ec53c961d739","url":"https://brett.cloud/ai-productivity/","title":"Productivity with AI","summary":"My experience with AI has been that it augments my abilities. It can consolidate information and automate tedium to make me more productive. For more sophisticated tasks related to programming, AI cannot replace a hardworking and fully engaged engineer.\nSome people might feel like we\u0026rsquo;re at a moment like this:\nSoftware developers in the 60s 🤭 pic.twitter.com/eZZYTmzl0E\n\u0026mdash; Ryan Els (@RyanEls4) January 16, 2025 And we could be. But, we\u0026rsquo;re absolutely not at a moment like this:","content_text":"My experience with AI has been that it augments my abilities. It can consolidate information and automate tedium to make me more productive. For more sophisticated tasks related to programming, AI cannot replace a hardworking and fully engaged engineer.\nSome people might feel like we\u0026rsquo;re at a moment like this:\nSoftware developers in the 60s 🤭 pic.twitter.com/eZZYTmzl0E\n\u0026mdash; Ryan Els (@RyanEls4) January 16, 2025 And we could be. But, we\u0026rsquo;re absolutely not at a moment like this:\n\u0026quot;ok claude, make a billion dollar b2b todo app. make no mistakes.\u0026quot; pic.twitter.com/mPE0gPFgac\n\u0026mdash; amrit (@amritwt) July 5, 2025 Then, there\u0026rsquo;s this:\nI met a founder today who said he writes 10,000 lines of code a day now thanks to AI. This is probably the limit case. He\u0026#39;s a hotshot programmer, he knows AI tools very well, and he\u0026#39;s talking about a 12 hour day. But he\u0026#39;s not naive. This is not 10,000 lines of bug-filled crap.\n\u0026mdash; Paul Graham (@paulg) August 7, 2025 Who in this scenario is actually confirming that it\u0026rsquo;s not 10k lines of slop?\nI\u0026rsquo;ve seen software engineers sometimes get defensive on the topic of AI, because they are perturbed by these sorts of unrealistic soundbites. Similar melodramatic sentiments are rampant on LinkedIn. I\u0026rsquo;ve been grateful to work with other engineers that have a strong interest in productivity and are level-headed about AI.\nThe reality is somewhere between the extremes. AI isn\u0026rsquo;t replacing engineers, but it\u0026rsquo;s also not just a fancy autocomplete. When used thoughtfully, it can genuinely accelerate development workflows by taking on specific, well-defined roles in the programming process.\nbrettinternet/ai AI tools, research \u0026amp; playground\nUsage In my currently evolving workflows, AI fulfills a few very specific pair programming roles to augment my work:\nCode completions Discovery Surgical updates Iterative edit-test loops There are MCP servers that assist with most of these roles. I have some I\u0026rsquo;m working on and a few I use regularly.\n1. Code completions This is the most obvious pair programming application for AI.\nCopilot code completions These are extremely context-aware changes and combat small-scale tedium.\n2. Discovery Discovery is my favorite usecase for AI. I use it for researching topics, summarizing documentation, querying libraries and codebases, getting usage examples, and planning implementation approaches. This is where AI shines as a research assistant that can quickly traverse large amounts of information.\nFor codebase exploration, AI excels at answering questions like \u0026ldquo;What are the side effects of this module?\u0026rdquo; or \u0026ldquo;Show me all the places where authentication is handled.\u0026rdquo; I frequently use this to understand hotspots in code and trace dependencies before making changes.\nWell-structured codebases with clear boundaries are easier for both humans and AI to navigate. When I refactored a large codebase using Context Boundaries for team scalability, it also improved AI\u0026rsquo;s ability to provide focused, relevant insights by confining context to specific code subdivisions.\nThis raises an important design question as we integrate AI into development workflows:\nHow can we improve code organization for both human and AI readability?\nThe answer benefits onboarding, knowledge transfer, and debugging regardless of whether you\u0026rsquo;re working with human teammates or AI assistants.\n3. Surgical updates AI can accomplish more sophisticated tasks when it\u0026rsquo;s steered towards a very specific context. I have a coworker that calls these \u0026ldquo;surgical updates\u0026rdquo;. This is where you pave a precise path for the agent to make specific changes. You might build up a context from a discovery or planning stage with an agent. In large enterprise codebases, this is how you manage context.\nClaude 4 just refactored my entire codebase in one call.\n25 tool invocations. 3,000+ new lines. 12 brand new files.\nIt modularized everything. Broke up monoliths. Cleaned up spaghetti.\nNone of it worked.\nBut boy was it beautiful. pic.twitter.com/wvmzh7IeAP\n\u0026mdash; vas (@vasumanmoza) May 25, 2025 Writing code is rarely the bottleneck. The real challenges in software development are understanding requirements, designing systems, debugging complex interactions, and making architectural decisions. Even with AI assistance, these cognitive tasks require human judgment, domain expertise, and the ability to reason about trade-offs. AI can help you write code faster, but it can\u0026rsquo;t replace the critical thinking needed to determine what code should be written in the first place.\nClaude code is closed-source but after some inspection you\u0026rsquo;ll find it ships with a few vendor distributions: (a) JetBrains extension, (b) VSCode extension, and (c) ripgrep.\n@anthropic/claude-code Ripgrep is a CLI tool for finding filenames and text in files with regex. A major differentiator between agents right now is how well they find relevant information and fill their context with precisely what\u0026rsquo;s needed.\nThe workflow might look like this:\nBuild up the context for what you\u0026rsquo;re working on, this is the rewind checkpoint Perform a task, but at a stopping point you should rewind (double escape) the context checkpoint You can do this with multiple chats (for Claude Code, run /resume and select the context checkpoint) Describe to the agent that your developer finished the task and to provide feedback Tip It appears LLMs provide more honest with feedback to a third party (e.g. \u0026ldquo;my developer\u0026rdquo;) 4. Iterative Edit-Test Loops flowchart LR A[🤖 Code] --\u0026gt; B[🧪 Test] B --\u0026gt; C[🔧 Fix] C --\u0026gt; A B --\u0026gt; D[✅ Done] style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff style B fill:#92400e,stroke:#f59e0b,stroke-width:2px,color:#ffffff style C fill:#991b1b,stroke:#ef4444,stroke-width:2px,color:#ffffff style D fill:#065f46,stroke:#10b981,stroke-width:2px,color:#ffffff AI agents are excellent at small tasks where they can iteratively loop through problems that provide immediate feedback. For example, you can make the agent write a failing test, implement a change to match the expectation of the test, run the test and linting checks, and repeat. Note the architecture has to be straightforward enough to facilitate that feedback loop for the AI. This is becoming easier with additional tooling, such as validating UI changes with the Playwright MCP.\nI\u0026rsquo;ve seen Claude delete or add @tag :skip for tests in order to get them to \u0026ldquo;pass\u0026rdquo;. Engineers have to be hands-on conductors. However, AI agents are excellent at setting up tests and other boilerplate and iterative test-driven development–just be sure to review that the coverage is meaningful.\nBest Practice Workflow Using an AI agent for development looks unique for different tasks. Let me lay out a very general workflow with agentic prompting and some ideas to guide our approach.\nCreate worktree as a sibling folder to work/repo-name to parallelize working on a repository Use Linear MCP to examine specifications of a ticket Investigate the work in parallel with the agent in main worktree, ask the agent for an execution plan and then analyze the plan Run a first pass on the work and write tests for our expectations (or inverse order) Review the work, refactor or fill in the gaps Caution You\u0026rsquo;ll discover within the first few minutes of using Claude that it consistently responds with this praise:\nYou\u0026rsquo;re absolutely right!\nGlazing is bad There was a GPT 4o update a few months ago where OpenAI released a personality update that was intensely sycophantic and mirrored user language. OpenAI\u0026rsquo;s AMA for the GPT5 release had users begging for the return of the 4o user engagement maximizer because it was \u0026ldquo;friendly\u0026rdquo;.\nWe need self-awareness about what using AI does to our psychology and good reviewing practices to avoid problematic code getting onto main.\nwait what pic.twitter.com/4CKClCEzvH\n\u0026mdash; Steve (Builder.io) (@Steve8708) November 14, 2024 I noticed a coworker published a PR for review that had invalid code and the engineer blamed it on AI. People are accountable for code. AI can\u0026rsquo;t be accountable.\nThere doesn\u0026rsquo;t need to be a major paradigm shift with best practice. We should still maintain all existing practices for code maintainability whether it\u0026rsquo;s generated by AI or written by humans. For example, of course we should be concerned about what code AI writes. The same is true when we select libraries or languages without AI. In both cases we own the decision and the code. Age old best practice continue even with modern AI technology.\nOpen Questions As LLMs and the tooling evolves, so do my workflows. I\u0026rsquo;m continuing to learn and grow with these changes. My AI repo is where I play with these tools and figure out how to apply them to other projects.\nCan engineers become excessively reliant on agentic prompting? Will this change engineering culture? What will this mean especially for newer programmers in the field?\nWill LLM innovation will begin to plateau? I wonder if we\u0026rsquo;re nearing a point where throwing more compute or a longer chain of thought won\u0026rsquo;t yield additional gains in performance.\nAre Anthropic and OpenAI subsidizing access to their models and will prices skyrocket soon? GPT5 appears to have been a cost-saving exercise for several reasons.\nFor now, AI can augment software engineering in meaningful ways and I encourage every software engineer to discover what LLMs can do for your workflows.\nConclusion AI isn\u0026rsquo;t going to replace thoughtful engineering, but it can make thoughtful engineers more effective. The key is approaching it as a sophisticated tool that excels in specific contexts such as code completion, research and discovery, focused updates, and iterative problem-solving. As the technology evolves, so should our practices for integrating it responsibly into development workflows.\nThis post was adapted from a lightning talk I gave to a group of executives.","banner_image":"https://brett.cloud/ai-productivity/claude-vendor_hu_8e63b9471987d1d1.png","date_published":"2025-08-11T00:00:00Z","date_modified":"2025-08-11T00:00:00Z"},{"id":"693f0c05-f0eb-5e65-59f3-397507b86f24","url":"https://brett.cloud/boundary/","title":"Scaling Elixir Applications with Context Boundaries","summary":"Building large-scale Elixir applications presents unique challenges. As codebases grow beyond the initial team of 3-5 developers, the lack of enforced architectural boundaries becomes a significant bottleneck. Teams spend more time reading and understanding code than writing new features, and the dynamic nature of Elixir makes code navigation increasingly difficult without proper structure.\nLately I\u0026rsquo;ve been exploring how context boundaries, enforced by compile-time checks, can transform unwieldy monoliths into well-organized, maintainable applications that can scale to teams of 20+ engineers.","content_text":"Building large-scale Elixir applications presents unique challenges. As codebases grow beyond the initial team of 3-5 developers, the lack of enforced architectural boundaries becomes a significant bottleneck. Teams spend more time reading and understanding code than writing new features, and the dynamic nature of Elixir makes code navigation increasingly difficult without proper structure.\nLately I\u0026rsquo;ve been exploring how context boundaries, enforced by compile-time checks, can transform unwieldy monoliths into well-organized, maintainable applications that can scale to teams of 20+ engineers.\nThe Scaling Problem Why Large Elixir Codebases Become Unwieldy When Elixir applications start small, the flexibility of the language is a tremendous asset. You can quickly prototype features, leverage pattern matching, and build robust systems with relatively few lines of code. However, as applications grow, several problems emerge:\nLack of Enforced Structure: Phoenix contexts provide organizational guidance, but there\u0026rsquo;s no compile-time enforcement preventing modules from calling deep into other contexts\u0026rsquo; internals. An organization module might directly access user database schemas, creating tight coupling that makes refactoring dangerous. Inconsistent Organization Patterns: Different teams or developers organize code differently. Some put business logic in controllers, others in contexts, and still others in custom service modules. These inconsistencies cause immense mental overhead for teams I\u0026rsquo;ve been on. Circular Dependencies: Without boundaries, it\u0026rsquo;s easy to create circular dependencies that slow compilation times in Elixir and make the code harder to reason about. Module A calls Module B, which calls Module C, which calls Module A. This creates a tangled web and slow CI. Testing Complexity: When modules are tightly coupled, testing becomes complex. You end up testing through the web API layer because it\u0026rsquo;s the only stable interface, leading to slow, brittle tests. Reading vs. Writing Code In mature applications or when onboarding in a new team, I find myself spending more time reading code than writing it. This ratio becomes problematic when functions are scattered across the codebase with no clear ownership and business logic is mixed with infrastructure concerns.\nOne approach to dealing with readability is to address organizational patterns for code, such as well-defined and enforceable \u0026ldquo;boundaries\u0026rdquo;.\nThe Boundary Library Boundary by Saša Jurić provides compile-time warnings of architectural boundaries in Elixir applications. It transforms organizational guidelines into compiler checks, catching boundary violations before they reach production.\nCopy mix clean \u0026amp;\u0026amp; \\ # Cleaning prevents false positives mix compile --warnings-as-errors Copy # count boundary errors mix compile | grep -E \u0026#39;warning:.*(boundary|forbidden reference)\u0026#39; | wc -l How Boundary Works The library operates through registering module attributes that define named groups of modules called boundaries. These boundaries can export inner modules from within a boundary and make them publicly accessible. Boundaries have dependencies from other boundary exports.\nHere\u0026rsquo;s a simple example:\nCopy defmodule MyApp.Catalog do @moduledoc \u0026#34;\u0026#34;\u0026#34; This boundary can use MyApp.Users and exports Products and Categories modules \u0026#34;\u0026#34;\u0026#34; use Boundary, deps: [ MyApp.Users ], exports: [ Products, # MyApp.Catalog.Products Categories, # MyApp.Catalog.Categories ] end defmodule MyApp.Catalog.Products do @moduledoc \u0026#34;This module is exported and can be used by other boundaries\u0026#34; end defmodule MyApp.Catalog.Internal.PriceCalculator do @moduledoc \u0026#34;This module is internal and cannot be used outside the boundary\u0026#34; end If module functions in another boundary try to call MyApp.Catalog.Internal.PriceCalculator, the compiler will raise an error.\nSetting Up Boundary Add Boundary to your mix.exs:\nCopy defp deps do [ {:boundary, \u0026#34;~\u0026gt; 0.10\u0026#34;, runtime: false}, ] end The runtime: false option is important. Boundary is a compile-time tool and doesn\u0026rsquo;t need to be included in your production release.\nContext Boundaries On my Elixir team, we\u0026rsquo;ve extended the ideas from the Boundary library into an opinionated design pattern we call \u0026ldquo;Context Boundaries\u0026rdquo;.\nWhat Are Context Boundaries? Context boundaries are architectural constraints that group related functionality together and define explicit interfaces for interaction between different parts of your application. Think of them as microservices within a monolith. Each boundary owns a specific business domain and exposes a well-defined API.\nIn an e-commerce application, you might have top-level boundaries like:\nCopy lib/ └── my_app/ ├─ billing/ # Payment processing, invoicing, subscriptions ├─ catalog/ # Product management, categories, pricing ├─ orders/ # Order processing, fulfillment, tracking ├─ users/ # Authentication, authorization, user profiles ├─ billing.ex ├─ catalog.ex ├─ orders.ex └─ users.ex Each boundary would have its own schemas, business logic, and database concerns, but could only interact with other boundaries through their public interfaces defined in the outer context file.\nIn larger organizations, boundaries naturally form around teams and interfaces would likely be used by other teams, so design them thoughtfully.\nBenefits of Context Boundaries Reduced Cognitive Load: Developers working on the catalog system don\u0026rsquo;t need to understand the intricacies of billing logic. They just need to know the public API. They should also spend less time figuring out where code lives. Parallel Development: Teams can shape themselves around business domains and can work independently on different boundaries without merge conflicts. Clearer Testing Strategy: Each boundary can be thoroughly tested in isolation, with integration tests covering the interactions between boundaries. Future Microservice Extraction: If you ever need to split off pieces of your monolith, boundaries provide natural seams for extraction. Documentation: The boundary definitions serve as living documentation of your system\u0026rsquo;s architecture. Boundaries are also good places to define actual documentation within a more narrow context for both humans and LLMs (e.g. CLAUDE.md). Improved Discoverability: Public interfaces make it clear what operations are available and how to use them. Performance: Boundaries should reduce compilation time by eliminating circular dependencies. Boundary checks happen at compile time, not runtime, so there\u0026rsquo;s no performance impact in production. An honest implementation of context boundaries should turn spaghetti code into a well-organized tree with each domain branching out in a visually traceable call stack.\nImplementing Context Boundaries There are some additional steps to take to maximize boundaries for larger organizations.\nDirectory Structure A well-organized boundary-based application follows a consistent directory structure. Copy lib/ my_app/ catalog/ products/ schemas/ product.ex actions/ list_products.ex create_product.ex update_product.ex categories/ schemas/ category.ex actions/ list_categories.ex utils/ price_calculator.ex catalog.ex orders/ order_processing/ schemas/ order.ex line_item.ex actions/ create_order.ex process_payment.ex fulfillment/ actions/ ship_order.ex track_shipment.ex orders.ex users/ authentication/ actions/ login.ex register.ex profiles/ schemas/ user.ex actions/ update_profile.ex users.ex Context Interface Design Each context exposes a clean public interface through its main module. Here\u0026rsquo;s how you might structure the Catalog context:\nCopy defmodule MyApp.Catalog do @moduledoc \u0026#34;\u0026#34;\u0026#34; Context for managing product catalog. Handles products, categories, pricing, and inventory. \u0026#34;\u0026#34;\u0026#34; @behaviour MyApp.Catalog.Products.Actions.ListProducts @behaviour MyApp.Catalog.Products.Actions.GetProduct @behaviour MyApp.Catalog.Products.Actions.CreateProduct @behaviour MyApp.Catalog.Products.Actions.UpdateProduct @behaviour MyApp.Catalog.Categories.Actions.ListCategories @behaviour MyApp.Catalog.Categories.Actions.CreateCategory use Boundary, deps: [MyApp.Users], exports: [Products.Schemas.Product, Categories.Schemas.Category] alias MyApp.Catalog.Products.Actions alias MyApp.Catalog.Categories.Actions, as: CategoryActions # Product operations @impl Actions.ListProducts defdelegate list_products(params \\\\ []), to: Actions.ListProducts, as: :list_products @impl Actions.GetProduct defdelegate get_product(id), to: Actions.GetProduct, as: :get_product @impl Actions.CreateProduct defdelegate create_product(attrs), to: Actions.CreateProduct, as: :create_product @impl Actions.UpdateProduct defdelegate update_product(product, attrs), to: Actions.UpdateProduct, as: :update_product # Category operations @impl Actions.ListCategories defdelegate list_categories(), to: CategoryActions.ListCategories, as: :list_categories @impl Actions.CreateCategory defdelegate create_category(attrs), to: CategoryActions.CreateCategory, as: :create_category end This interface provides several benefits:\nDiscoverability: All public operations are visible in one place Delegation: Implementation details are hidden in specialized modules Consistency: All contexts follow the same interface pattern Type Safety: You can use behaviors to ensure implementations match @callback interfaces Mockable: Behaviour definitions make each action easily mocked Action Modules Action modules contain the actual business logic and should follow a consistent pattern. They expose and implement a callback for their interface. At the org I work at now, we generally have one file contain only one action so that an action has one public function but can have unlimited private functions.\nCopy defmodule MyApp.Catalog.Products.Actions.CreateProduct do @moduledoc false @behaviour __MODULE__ alias MyApp.Catalog.Products.Schemas.Product alias MyApp.Users alias MyApp.Repo @type params_t :: %{ name: String.t(), description: String.t(), price: Decimal.t(), category_id: String.t(), created_by: String.t() } @type result_t :: {:ok, Product.t()} | {:error, Ecto.Changeset.t()} @doc \u0026#34;\u0026#34;\u0026#34; Creates a new product. \u0026#34;\u0026#34;\u0026#34; @callback create_product(params_t()) :: result_t() def create_product(params) do with {:ok, user} \u0026lt;- Users.get_user(params.created_by), :ok \u0026lt;- validate_permissions(user), {:ok, product} \u0026lt;- create_product(params) do {:ok, product} end end defp validate_permissions(user) do if Users.has_permission?(user, :create_products) do :ok else {:error, :unauthorized} end end defp create_product(params) do %Product{} |\u0026gt; Product.changeset(params) |\u0026gt; Repo.insert() end end The @behaviour __MODULE__ is a self-referencing behavior that replaces @spec with @callback to provide compile-time documentation of our action functions in our context API. Each action module is now a swappable backend that can be easily mocked in tests or refactored.\nKey principles for action modules that we\u0026rsquo;ve implemented with our large Elixir monolith:\nSingle Responsibility: One action per module with self-referencing behavior Public/Private Separation: One public function that matches the module name (a uniform function name such as call might also be appropriate) alongside private helpers Type Specifications: Clear input/output types with @callback specification defining the contract of the action\u0026rsquo;s behaviour Error Handling: Consistent error patterns across actions that mask errors which could be useless outside the context Schema Organization Schemas should be pure data definitions with minimal logic and only pure functions:\nCopy defmodule MyApp.Catalog.Products.Schemas.Product do use Ecto.Schema import Ecto.Changeset @type t :: %__MODULE__{ id: String.t(), name: String.t(), description: String.t(), price: Decimal.t(), category_id: String.t(), inserted_at: DateTime.t(), updated_at: DateTime.t() } schema \u0026#34;products\u0026#34; do field :name, :string field :description, :string field :price, :decimal belongs_to :category, MyApp.Catalog.Categories.Schemas.Category timestamps() end def changeset(product, attrs) do product |\u0026gt; cast(attrs, [:name, :description, :price, :category_id]) |\u0026gt; validate_required([:name, :price, :category_id]) |\u0026gt; validate_number(:price, greater_than: 0) |\u0026gt; foreign_key_constraint(:category_id) end end Schemas are typically exported by boundaries since they\u0026rsquo;re part of the public interface. Other contexts will likely need to pattern match on them and pass them around. You may want to bulk export schemas in an inner boundary.\nCopy # lib/my_app/catalog/products/schemas.ex defmodule MyApp.Catalog.Products.Schemas do @moduledoc \u0026#34;\u0026#34;\u0026#34; Product schemas boundary \u0026#34;\u0026#34;\u0026#34; use Boundary, deps: [ MyApp.Catalog.Categories.Schemas ], exports: [ Product # Ecto schema definition ] end This is one example of how boundaries interface with each other.\nCross-Boundary Communication Synchronous Communication For synchronous operations where you need immediate results, call through the boundary\u0026rsquo;s public interface:\nCopy defmodule MyApp.Orders.Actions.CreateOrder do @moduledoc false @behaviour __MODULE__ alias MyApp.Catalog alias MyApp.Orders.Schemas.Order alias MyApp.Users @callback create_order(map()) :: {:ok, Order.t()} | nil def create_order(params) do with {:ok, user} \u0026lt;- Users.get_user(params.user_id), {:ok, products} \u0026lt;- validate_products(params.line_items), {:ok, order} \u0026lt;- create_order(user, products, params) do {:ok, order} end end defp validate_products(line_items) do product_ids = Enum.map(line_items, \u0026amp; \u0026amp;1.product_id) # Call through Catalog boundary interface case Catalog.get_products_by_ids(product_ids) do {:ok, products} when length(products) == length(product_ids) -\u0026gt; {:ok, products} _ -\u0026gt; {:error, :invalid_products} end end end Asynchronous Communication For side effects that don\u0026rsquo;t need immediate consistency, use event-driven communication:\nCopy defmodule MyApp.Orders.Actions.CompleteOrder do @moduledoc false @behaviour __MODULE__ alias MyApp.EventBus alias MyApp.Orders.Schemas.Order @type complete_order(String.t()) :: {:ok, Order.t()} def complete_order(order_id) do with {:ok, order} \u0026lt;- get_order(order_id), {:ok, updated_order} \u0026lt;- mark_completed(order) do # Publish event for other contexts to react Phoenix.PubSub.broadcast(MyApp.PubSub, \u0026#34;order_completed:#{updated_order.user_id}\u0026#34;, %{ order_id: updated_order.id, user_id: updated_order.user_id, total: updated_order.total }) {:ok, updated_order} end end end Web Layer Organization The web layer should also define its own boundary in my_app_web.ex.\nCopy deps: [ Absinthe.Subscription, Absinthe, MyApp.Billing, MyApp.Catalog, MyApp.Orders, MyApp.Users, Phoenix, ], exports: [ Endpoint ] Keep controllers or resolvers thin. For example, a GraphQL endpoint\u0026rsquo;s resolver should only handle request-specific concerns. Similarly, controllers should delegate business logic to contexts.\nChallenges There are a few non-trivial challenges we encountered at our org with our implementation. The initial refactor was significant overhead since moving files around and function calls was so wide-spread. We were all hands on deck for a few weeks to make this work. Following our initial refactor, continuing education and discipline became a priority for us.\nDatabase queries that span boundaries become more complex and may require boundary coupling, duplication or event-driven updates. We\u0026rsquo;ve elected to use a controlled coupling approach for interdependent schemas and joins. Each domain has a dedicated schemas module that acts as a child boundary that exports all of its schemas.\nCopy # lib/my_app/orders/order_processing/schemas.ex exports: [ Order, # Ecto schema definition LineItem # Ecto schema definition ] Copy deps: [ MyApp.Orders.OrderProcessing.Schemas ] We may eventually have to decouple some domains that need to become their own microservices. This needs to be balanced against architectural benefits.\nConclusion The dynamic nature and loose conventions of Elixir makes it challenging to navigate large codebases. Context boundaries enforced through the Boundary library provide a practical path for scaling Elixir applications beyond small teams.\nWhether you\u0026rsquo;re currently feeling the pain of a growing monolith or planning for future scale, context boundaries offer a pragmatic solution that preserves the benefits of monolithic deployment while providing the organizational clarity of business domains and microservices.","date_published":"2025-08-07T00:00:00Z","date_modified":"2025-08-07T00:00:00Z"},{"id":"448f9db5-2278-5403-6903-a9d8fabed80f","url":"https://brett.cloud/appresize/","title":"Appresize","summary":" Appresize Appresize is a utility to resize and move apps from anywhere on the window with custom modifiers and other preferences.\nAlthough not captured by the screen recorder, the cursor does follow window Custom modifiers \u0026amp; other behaviors One of my favorite abandonware apps on macOS was an old closed-source Objective C application called Hyperdock that had a small secondary feature to resize and move windows by a modifier from anywhere on the window.","content_text":" Appresize Appresize is a utility to resize and move apps from anywhere on the window with custom modifiers and other preferences.\nAlthough not captured by the screen recorder, the cursor does follow window Custom modifiers \u0026amp; other behaviors One of my favorite abandonware apps on macOS was an old closed-source Objective C application called Hyperdock that had a small secondary feature to resize and move windows by a modifier from anywhere on the window.\nThere are various window utilities on Mac, but none of them satisfied my very specific expectation. Now, this demand lives on with Swift in a small utility here.","banner_image":"https://brett.cloud/appresize/demo_hu_fb936eab2e112abc.gif","date_published":"2025-07-25T00:00:00Z","date_modified":"2025-07-25T00:00:00Z"},{"id":"0c3d20fb-c27a-56a6-79f0-b0620d3ad91f","url":"https://brett.cloud/stream-deck-mount/","title":"Magnetic Stream Deck mount for 1/4-inch screw","summary":" Stream Deck The Stream Deck is an excellent tool to invoke shortcuts. Check out my Hammerspoon config which I use to extend the Stream Deck scripting capabilities.","content_text":" Stream Deck The Stream Deck is an excellent tool to invoke shortcuts. Check out my Hammerspoon config which I use to extend the Stream Deck scripting capabilities.\nBuild Add a pause to the layer to add the magnetics just before the last layer of the enclosure. Add 12 of the 6x3mm round magnets into place before continuing the print.\nYour browser does not support the video tag. When the print is complete, use a soldering iron to heat the threaded insert and melt into the center hole. I\u0026rsquo;ve included a smaller print with the hole to practice putting in the threaded insert.\nMakerWorld 3D model STL/CAD Files and instructions\nResult ","banner_image":"https://brett.cloud/stream-deck-mount/mount_hu_7f30bb1f4c96d724.gif","date_published":"2024-12-18T00:00:00Z","date_modified":"2024-12-18T00:00:00Z"},{"id":"4b0a409c-fbe5-58b9-a9e5-67d8baee7541","url":"https://brett.cloud/slackbot/","title":"Slackbot","summary":"Features \u0026ldquo;Obituaries\u0026rdquo; to get notified when users are removed (or added) from the Slack organization Configurable chat responses and reactions AI Chat with a variety of prompted, unhinged, and sticky personas per user Configurable by environment variables and yaml; secrets can also be mounted to a file Prebuilt containers ready for deployment and example docker deployment to handle public event endpoint brettinternet/slackbot Slack utilities for the workplace","content_text":"Features \u0026ldquo;Obituaries\u0026rdquo; to get notified when users are removed (or added) from the Slack organization Configurable chat responses and reactions AI Chat with a variety of prompted, unhinged, and sticky personas per user Configurable by environment variables and yaml; secrets can also be mounted to a file Prebuilt containers ready for deployment and example docker deployment to handle public event endpoint brettinternet/slackbot Slack utilities for the workplace","banner_image":"https://brett.cloud/slackbot/user-status_hu_d0b94cfa612cf06.png","date_published":"2024-05-19T00:00:00Z","date_modified":"2024-05-19T00:00:00Z"},{"id":"0130099a-8d8b-59bd-e933-3f18cc932985","url":"https://brett.cloud/zfs-passthrough/","title":"The Stages of ZFS Data Loss Grief","summary":"I use a widely-known and inexpensive method to add additional SATA storage with the Dell Perc H310. I found this old Host Bus Adapter (HBA) a long while back. This HBA can be flashed to IT mode by taping over a couple PCI pins to bypass the hardware RAID and use software RAID. 1\nSince moving my home servers to Proxmox to manage virtualization, I setup disk passthrough to a VM managing my ZFS array. What could go wrong?","content_text":"I use a widely-known and inexpensive method to add additional SATA storage with the Dell Perc H310. I found this old Host Bus Adapter (HBA) a long while back. This HBA can be flashed to IT mode by taping over a couple PCI pins to bypass the hardware RAID and use software RAID. 1\nSince moving my home servers to Proxmox to manage virtualization, I setup disk passthrough to a VM managing my ZFS array. What could go wrong?\nCopy $ qm set 100 -scsi1 /dev/disk/by-id/… $ qm set 100 -scsi2 /dev/disk/by-id/… $ … I\u0026rsquo;m sure this is fine.\nHours later, after a seemingly innocent reboot…\nOn the guest:\nCopy $ zpool list no pools available On the host:\nCopy $ zpool list no pools available Hmm.\n1. Denial Copy $ zpool import tank cannot import \u0026#39;tank\u0026#39;: I/O error Destroy and re-create the pool from a backup source. Uh oh.\nCopy $ zpool import -F tank cannot import \u0026#39;tank\u0026#39;: one or more devices is currently unavailable This is not good.\n2. Anger My restic backups are stale because of some issues with my homelab. 🤦‍♂️\nCopy $ zpool import -N -o readonly=on -f tank cannot import \u0026#39;tank\u0026#39;: I/O error Destroy and re-create the pool from a backup source. Destroy and re-create the pool from\ta backup source.\nAt this point, most forums appear to suggest that the pool is lost forever.\n3. Bargaining Readonly should have worked 🤔\nCopy $ zpool import -N -o readonly=on -f -R tank pool: tank id: … state: ONLINE status: Some supported features are not enabled on the pool. (Note that they may be intentionally disabled if the \u0026#39;compatibility\u0026#39; property is set.) action: The pool can be imported using its name or numeric identifier, though some features will not be available without an explicit \u0026#39;zpool upgrade\u0026#39;. config: tank ONLINE raidz2-0 ONLINE … ONLINE … ONLINE … ONLINE … ONLINE Copy $ zpool import -F Same output as above.\nOnline seems good, right?\nCopy $ zpool status no pools available Copy $ zpool import -F -m tank cannot import \u0026#39;tank\u0026#39;: one or more devices is currently unavailable Well, here we go. Let\u0026rsquo;s find the txg to use for a rollback.\nCopy $ zpool import -FX tank # seemingly hanging for a while… ^C^C^C^C That option must not work the way I expected (forgive my impatience, dear reader).\n4. Depression At this point I pull down the latest snapshot from Backblaze and assess the damage.\nCopy $ zdb tank zdb: can\u0026#39;t open \u0026#39;tank\u0026#39;: No such file or directory ZFS_DBGMSG(zdb) START: ZFS_DBGMSG(zdb) END What have I done to myself.\n5. Acceptance Copy $ restic snapshots repository … opened (version 2, compression level auto) ID Time Host Tags Paths -------------------------------------------------------------------------- 20ee6d7b … restic-remote restic /data Deep breath.\nCopy $ restic restore 20ee6d7b --target ./data 6. ? Ok, wait a minute. Let\u0026rsquo;s try that mysterious -X flag again, but with more patience.\nCopy $ zpool import -FX tank # … waiting … staring … go get dinner … waiting … put baby to bed … Exit 0! It worked!\nCopy $ ls /mnt/tank files files files! Immediately:\nCopy $ rsync -ahP /mnt/tank elsewhere:/mnt/pond/tank I later discovered from some folks on a ZFS forum that\u0026rsquo;s better to avoid disk passthrough for ZFS pools in VMs, but this may depend on the HBA controller.\nNow, I pass the entire HBA controller to guest VMs instead of individual disks when using ZFS. Lesson learned.\nThank you FreeBSD, Truenas, r/zfs communities and datahoarders.\n1: There are several instructions to flash the Dell Perc H310 HBA to IT mode: video walkthrough, ServeTheHome post, and TrueNAS forum thread","date_published":"2023-12-09T00:00:00Z","date_modified":"2023-12-09T00:00:00Z"},{"id":"2dd8becd-9037-531a-f966-555334dcc916","url":"https://brett.cloud/tabbed/","title":"Tabbed extension","summary":" Tabbed Tabbed is an experiment with a Chrome extension to visualize and organize browser tabs.\nDrag and drop browser session management Tabbed was built with accessibility in mind. See the full feature list and source or the latest releases.","content_text":" Tabbed Tabbed is an experiment with a Chrome extension to visualize and organize browser tabs.\nDrag and drop browser session management Tabbed was built with accessibility in mind. See the full feature list and source or the latest releases.\nView in Chrome Web Store ","banner_image":"https://brett.cloud/tabbed/icon_hu_c5f4dd98a915af4d.png","date_published":"2023-04-03T00:00:00Z","date_modified":"2023-04-03T00:00:00Z"},{"id":"241d319e-7f12-5c53-2982-d0d38ee5ec95","url":"https://brett.cloud/mic-mute/","title":"Mic Mute for macOS","summary":" Mic Mute for macOS Mic Mute is a system-wide mute for macOS with a global shortcut and a clear visual indicator. It\u0026rsquo;s inspired by VCM for Windows.\nMute with the shortcut Cmd Shift A or from the system tray dropdown menu.\nMute window follows cursor to screens and monitors The mute indicator window will follow the cursor to desktops or screens and monitors. The system tray icon will also indicate the mute status. Once microphones are on again, the window will hide. View releases.","content_text":" Mic Mute for macOS Mic Mute is a system-wide mute for macOS with a global shortcut and a clear visual indicator. It\u0026rsquo;s inspired by VCM for Windows.\nMute with the shortcut Cmd Shift A or from the system tray dropdown menu.\nMute window follows cursor to screens and monitors The mute indicator window will follow the cursor to desktops or screens and monitors. The system tray icon will also indicate the mute status. Once microphones are on again, the window will hide. View releases.\nMuted and unmuted indicators:\nUpdate: What I use now My journey finding the best solution for this has evolved. I now use Hammerspoon which is a delightful way to hack together MacOS scripts. I store my configuration in my dotfiles. The Lua scripting in Hammerspoon makes maintaining this a lot more fun than foreign functions in Rust. I\u0026rsquo;m still able to use the hotkey ⌘ ⇧ A and I\u0026rsquo;ve also mapped it to a macropad that uses F13. My script also supports Push To Talk.\nHere\u0026rsquo;s the solution:\nCopy muteAlertId = nil -- Clear the alert if exists to avoid notifications stacking local function clearMuteAlert() if muteAlertId then hs.alert.closeSpecific(muteAlertId) end end -- Hold the hotkey for Push To Talk local holdingToTalk = false local function pushToTalk() holdingToTalk = true local audio = hs.audiodevice.defaultInputDevice() local muted = audio:inputMuted() if muted then clearMuteAlert() muteAlertId = hs.alert.show(\u0026#34;🎤 Microphone on\u0026#34;, true) audio:setInputMuted(false) end end -- Toggles the default microphone\u0026#39;s mute state on hotkey release -- or performs PTT when holding down the hotkey local function toggleMuteOrPTT() local audio = hs.audiodevice.defaultInputDevice() local muted = audio:inputMuted() local muting = not muted if holdingToTalk then holdingToTalk = false audio:setInputMuted(true) muting = true else audio:setInputMuted(muting) end clearMuteAlert() if muting then muteAlertId = hs.alert.show(\u0026#34;📵 Microphone muted\u0026#34;) else muteAlertId = hs.alert.show(\u0026#34;🎤 Microphone on\u0026#34;) end end -- `⌘ ⇧ A` but you could also map to F13 for a macropad hs.hotkey.bind({\u0026#34;cmd\u0026#34;, \u0026#34;shift\u0026#34;}, \u0026#34;a\u0026#34;, nil, toggleMuteOrPTT, pushToTalk) Hammerspoon also makes it incredibly easy to add other utilities, such as making a hotkey to change the default audio output or input. Find the latest source in my dotfiles.\nView dotfiles ","banner_image":"https://brett.cloud/mic-mute/alert-off_hu_b12b33d93a296b97.png","date_published":"2023-03-11T00:00:00Z","date_modified":"2023-03-11T00:00:00Z"},{"id":"901c7f5a-0779-5b5d-d968-717f1b5ec8a1","url":"https://brett.cloud/self-hosted/","title":"You can self-host","summary":"Remember the iPhone commercial from the late 2000s which introduced Apple\u0026rsquo;s AppStore with the phrase, \u0026ldquo;There\u0026rsquo;s an app for that\u0026rdquo;? That\u0026rsquo;s how open source services are now. There\u0026rsquo;s a wide selection of useful and mature software that containerization has made exceptionally portable.\nMy own homelab has become a monorepo of DevOps overkill, but self-hosting can be simple and easy with Docker. You can securely host applications with a cheap desktop in your home with minimal effort and a single docker-compose configuration file.","content_text":"Remember the iPhone commercial from the late 2000s which introduced Apple\u0026rsquo;s AppStore with the phrase, \u0026ldquo;There\u0026rsquo;s an app for that\u0026rdquo;? That\u0026rsquo;s how open source services are now. There\u0026rsquo;s a wide selection of useful and mature software that containerization has made exceptionally portable.\nMy own homelab has become a monorepo of DevOps overkill, but self-hosting can be simple and easy with Docker. You can securely host applications with a cheap desktop in your home with minimal effort and a single docker-compose configuration file.\nDemo I\u0026rsquo;ve set up a simple demo to host an application.\nDocker compose offers a very simple way to run and maintain self-hosted homelab. The configuration is portable, easy to understand, and a container orchestration can be run on a single node with just one command. As I demonstrate here, the available tooling makes DNS and proxying automation and the service setup very easy.\nThis demo hosts a simple Elixir notebook application called Livebook. I work in an Elixir shop where Livebook is a local favorite. Livebook uses notebooks similar to Python\u0026rsquo;s Jupyter except it\u0026rsquo;s built with Elixir and has real-time syncing between clients because it\u0026rsquo;s built on the Phoenix framework\u0026rsquo;s library LiveView.\nThis demo will set up a Cloudflared tunnel connection, a Traefik reverse proxy and the Livebook app. Cloudflare DNS is automated with CNAME creation from Traefik routes. There\u0026rsquo;s no port forwarding required to host this app on a domain you own.\nHere\u0026rsquo;s a sketch of the architecture:\ngraph TB tf(Terraform) -.- dns tf -.- argo dns{Cloudflare DNS} --\u0026gt; argo argo((Cloudflare Tunnels)) == Tunnel ==\u0026gt; cloudflared ddns -.- dns subgraph lan[Docker Network] style lan stroke-dasharray: 5 5 cloudflared --\u0026gt; traefik[Traefik reverse proxy] traefik --\u0026gt; livebook[Livebook] ddns[cloudflare-companion] -. service discovery .- livebook end The phony make targets below are used to simplify each step. Look at the Makefile to see what each one does.\nSetup First, initialize the config file and terraform project.\nCopy make setup This creates a .env file which you should edit with your own secrets. CLOUDFLARE_API_TOKEN needs Zone.DNS and Account.Cloudflare Tunnel write permissions for the domain in use. Use an API token, not an API key. The value for CLOUDFLARE_TUNNEL_TOKEN will come later.\nThen, create the Cloudflared tunnel. You\u0026rsquo;ll need Terraform, unless you create it from the Cloudflare Zero Trust dashboard. Note, using the dashboard setup, point the tunnel endpoint to http://traefik:80 as the cloudflared image sees the host within the docker network.\nCopy make terraform This plans and applies the terraform tunnel configuration. It creates a CNAME record tunnel.example.com that points to the Cloudflared tunnel URL.\nFind the tunnel_token value in the terraform output file ./tunnel/terraform.tfstate and add it as the value of CLOUDFLARE_TUNNEL_TOKEN.\nRun Start the docker compose.\nCopy make start This runs docker-compose --compatibility up. The compatibility flag appears to be required in order to set resource limits in docker-compose.\nYou can self-host Self-hosting is a satisfying hobby with amazing utility. These methods also provide ways to try out new technologies, host a simple blog, or make use of existing services that you find on GitHub. Let me know if the demo has helped you along with your own homelab.","banner_image":"https://brett.cloud/self-hosted/stay_at_home_server_hu_cd4bad5eca6ed8d6.jpg","date_published":"2023-01-13T00:00:00Z","date_modified":"2023-01-13T00:00:00Z"},{"id":"69240996-3e92-50ea-193a-0347aa88cd9a","url":"https://brett.cloud/homeops/","title":"Homeops","summary":"Don\u0026rsquo;t be fooled, having a home server is really just hundreds of hours of badblocks.\nI\u0026rsquo;ve been hard at work recently converting my architecture to support orchestrated deployments with multiple nodes. I haven\u0026rsquo;t quite achieved high-availability, and I\u0026rsquo;m not likely to take things that far. However, it has been an excellent journey to become more acquainted with Kubernetes.\nSetup and usage are inspired by a homelab gitops template and the k8s-at-home community. You can find similar setups with the k8s at home search. Historical revisions of my homelab setup had rootless Podman containers deployed with ansible as systemd units. Prior to that, I used docker-compose to orchestrate containers on a single node.","content_text":"Don\u0026rsquo;t be fooled, having a home server is really just hundreds of hours of badblocks.\nI\u0026rsquo;ve been hard at work recently converting my architecture to support orchestrated deployments with multiple nodes. I haven\u0026rsquo;t quite achieved high-availability, and I\u0026rsquo;m not likely to take things that far. However, it has been an excellent journey to become more acquainted with Kubernetes.\nSetup and usage are inspired by a homelab gitops template and the k8s-at-home community. You can find similar setups with the k8s at home search. Historical revisions of my homelab setup had rootless Podman containers deployed with ansible as systemd units. Prior to that, I used docker-compose to orchestrate containers on a single node.\nSetup Here have been some of my goals:\nFlux GitOps with this repository (cluster directory) Ansible node provisioning and K3s setup (Ansible roles and playbooks) Terraform DNS records (terraform) SOPS secrets stored in Git Renovate bot dependency updates WireGuard VPN pod gateway via paid service WireGuard VPN proxy hosted on VPS Cloudflared HTTP tunnel K8s gateway for local DNS resolution to cluster and NGINX ingress controller Both internal \u0026amp; external services with a service gateway OIDC authentication with LDAP Automatic Cloudflare DNS updates MetalLB bare metal K8s network loadbalancing Calico CNI ZFS JBOD mergerfs union NFS with SnapRAID backup for low-touch media files Restic backups to remote and local buckets go-task shorthand for useful commands (Taskfile and taskfiles) Some questions Why use ECC RAM? Hacker News discussion If you love your data, use ECC RAM Error rates increase rapidly with rising altitude Hardware I finally upgraded my media server chassis to a Supermicro CSE-826. For almost 7 years I was using a Node 804, which is popular among hobbyists because it fits 8x 3.5\u0026quot; drivers. I use old desktop hardware for this NAS and other nodes.\nI used a widely-known and inexpensive method to add additional SATA storage via a Host Bus Adapter (HBA). I purchased a Dell Perc H310 a long while back. Mine did come from overseas, but it turned out to be legit. This video shows how it can be flashed to an LSI 9211-8i IT (it\u0026rsquo;s called IT mode; see also 1, 2).\nHere are other recommended controllers.","banner_image":"https://brett.cloud/homeops/k8s_hu_ae5ca7ce8af1ed14.gif","date_published":"2022-11-02T00:00:00Z","date_modified":"2022-11-02T00:00:00Z"},{"id":"e68fe3a3-8874-56e1-c9d3-c5ecfbf71717","url":"https://brett.cloud/dotfiles/","title":"Dotfiles","summary":"My dotfiles are easy to setup for both desktop consoles and headless servers. I use Make, bash scripts and dotbot, an idempotent python script which configures directories, creates symlinks and run postscripts for Linux and macOS. I\u0026rsquo;ve broken the dotbot configuration out into multiple modules to select features suited for different environments.\nI\u0026rsquo;ve simplifed my nvim and emacs configurations by just using Astronvim and Doom respectively. While I love i3, I\u0026rsquo;m in the process of configuring Sway to see if I can match my productivity on Wayland.","content_text":"My dotfiles are easy to setup for both desktop consoles and headless servers. I use Make, bash scripts and dotbot, an idempotent python script which configures directories, creates symlinks and run postscripts for Linux and macOS. I\u0026rsquo;ve broken the dotbot configuration out into multiple modules to select features suited for different environments.\nI\u0026rsquo;ve simplifed my nvim and emacs configurations by just using Astronvim and Doom respectively. While I love i3, I\u0026rsquo;m in the process of configuring Sway to see if I can match my productivity on Wayland.\nDotfiles MacOS \u0026amp; Linux environments","banner_image":"https://brett.cloud/dotfiles/i3_hu_77a27c70e92b09d5.png","date_published":"2021-10-27T00:00:00Z","date_modified":"2021-10-27T00:00:00Z"},{"id":"1b0bc974-10e4-5e7e-89b0-924fce4212f1","url":"https://brett.cloud/hackintosh/","title":"Building a Hackintosh","summary":"Apple\u0026rsquo;s overpriced offering is moving away from modular, upgradable hardware. Some recent unveils of the iMac, Mac Pro, and MacBook Pro failed to meet the expectations of many professionals that demand improved specifications. Using the more powerful hardware available with macOS can be the ultimate environment for a developer—Unix coupled with strong machinery. For example, at the time of writing this, I\u0026rsquo;m on my Kaby Lake build, but current Apple hardware doesn\u0026rsquo;t offer the newest Intel generation. This is currently experimental in the community, but more stable options do exist with Skylake hardware.","content_text":"Apple\u0026rsquo;s overpriced offering is moving away from modular, upgradable hardware. Some recent unveils of the iMac, Mac Pro, and MacBook Pro failed to meet the expectations of many professionals that demand improved specifications. Using the more powerful hardware available with macOS can be the ultimate environment for a developer—Unix coupled with strong machinery. For example, at the time of writing this, I\u0026rsquo;m on my Kaby Lake build, but current Apple hardware doesn\u0026rsquo;t offer the newest Intel generation. This is currently experimental in the community, but more stable options do exist with Skylake hardware.\nI first started developing on a MacBook. There are aspects of Apple\u0026rsquo;s laptops that other hardware manufactures have not been able to match for me quite yet. But as for a desktop where form factor, sleek aluminum design, and the responsive trackpad are not concerns, then a Hackintosh is the way to go for tabletop computing.\nShould you build a hackintosh? Probably not. It\u0026rsquo;s a breach of Apple\u0026rsquo;s macOS Terms and Conditions and they have sued companies that attempt to turn a profit. However, Apple has been kind and hasn\u0026rsquo;t litigated individuals. Building a Hackintosh involves a lot of troubleshooting and tinkering. But if you\u0026rsquo;re okay with all of this, then try it out! The first place to start is to begin lurking in the community (/r/hackintosh).\nI\u0026rsquo;ve posted a guide to my most recent build where I use the latest Kaby Lake Intel processor and the latest mobo. I use an NVMe SSD, bluetooth accessories, iMessage, and other necessary features. My passion for Hackintosh fluctuates with my availability, but you can see my builds, guides and troubleshooting research at brettinternet/hackintosh.\nFeb 2019 update: Proprietary software can be frustrating and in privacy matters and developer support Apple appears to be straying further from the light. I\u0026rsquo;ve archived my personal Hackintosh project and moved on to Linux.","banner_image":"https://brett.cloud/hackintosh/system-info_hu_74bedefe8616fd9a.png","date_published":"2017-04-23T00:00:00Z","date_modified":"2017-04-23T00:00:00Z"},{"id":"85242e44-ec94-5232-d9da-90300358d0a5","url":"https://brett.cloud/notes/","title":"Manage notes with markdown and git","summary":"In school, to keep up with my peers, I took meticulous notes. I read and reread assignments, and I constantly evaluated my priorities to stay on task. I became obsessed with managing my productivity. I\u0026rsquo;ve used feature-rich apps like OmniFocus or Wunderlist and simpler tools like Apple Reminders.\nFor a while I used Evernote for note-taking, then I tried OneNote, Apple Notes and even Google Keep. I left Evernote because they\u0026rsquo;d seemed to have lost their way with gimmicky features. I liked that OneNote didn\u0026rsquo;t force-feed a premium option and it was just as accessible as Evernote. Later, I thought that maybe Apple Notes would be just enough for me, but I eventually found that it wasn\u0026rsquo;t better for anything other than lists or using the touch draw feature.","content_text":"In school, to keep up with my peers, I took meticulous notes. I read and reread assignments, and I constantly evaluated my priorities to stay on task. I became obsessed with managing my productivity. I\u0026rsquo;ve used feature-rich apps like OmniFocus or Wunderlist and simpler tools like Apple Reminders.\nFor a while I used Evernote for note-taking, then I tried OneNote, Apple Notes and even Google Keep. I left Evernote because they\u0026rsquo;d seemed to have lost their way with gimmicky features. I liked that OneNote didn\u0026rsquo;t force-feed a premium option and it was just as accessible as Evernote. Later, I thought that maybe Apple Notes would be just enough for me, but I eventually found that it wasn\u0026rsquo;t better for anything other than lists or using the touch draw feature.\nNow as a developer, I use markdown in my text editor. This process for note-taking certainly isn\u0026rsquo;t novel – you\u0026rsquo;ll even find books on GitHub written with markdown. Markdown offers the most formatting versatility, especially to display code excerpts with syntax highlighting and quickly reference links, create lists, and break notes out by sections. This is especially true since GitHub\u0026rsquo;s flavoring and small enhancements have made markdown previewing so accessible. Even this blog is made using markdown.\nMarkdown I love markdown. I like how widely supported it is, and I like how clear the markdown format is for conveying information. Markdown has become the ubiquitous plain text format for the GitHub community. My own blog posts are written in markdown (brettinternet.github.io). Composing markdown is a timeless method that isn\u0026rsquo;t tied to any third-party tools except a modern text editor (which isn\u0026rsquo;t going anywhere).\nIt\u0026rsquo;s true, this method isn\u0026rsquo;t perfect. I need separate tools for sketches and PDF markups. But these mediums make up about 0.5% of all my notes. I just use my NextCloud for storing files or Keybase for my public documents.\nGitHub has formalized their CommonMark specification to standardize GitHub\u0026rsquo;s flavor of markdown. Commonmark was put together by a few representatives of some major companies that use markdown like GitHub, Reddit, and Stack Exchange. Some nonconforming markdown features have added some very nice features such as task lists, autolinking, diff and code block improvements.\nGit I maintain a separate GitHub repository for my private notes and another for my public notes. Eventually, I\u0026rsquo;d like to self-host my own GitLab within my homelab.\nMy commit messages are short, but I attempt to make them meaningful so I can see my note history and refer back to find deleted excerpts. I also use aliases for git commands to make pushing to master very fast.\nDesktop I use Vim and VS Code for note-taking. Taking notes in my editor environment is ideal since I\u0026rsquo;m most comfortable with the shortcuts. VS Code offers a markdown previewer out of the box with Ctrl + Shift + V. Markdown\u0026rsquo;s lifespan on desktop will exceed my own, so I\u0026rsquo;m no longer bound to an app\u0026rsquo;s survival on a platform to access my notes.\nShareX is a good option for screenshots on Windows, while Flameshot works well on Linux. I tend to think that a simple bookmark is better than web clippings for most use cases.\nMobile Before I found Working Copy, a solid mobile git solution for iOS was the missing piece for keeping notes in a git repository. There are equally good options on Android like MGit or Termux. Working Copy offers free repository cloning and viewing, with push capabilities available through in-app purchase. The app includes SSH key support, syntax highlighting, and markdown preview. It\u0026rsquo;s an excellent mobile git client for iOS.\nSave Menu Everything not saved will be lost.\n- Nintendo \u0026ldquo;Quit Screen\u0026rdquo; message\nLike an accessible save menu, I keep everything in markdown. I record Docker commands and obscure command-line arguments, JavaScript problems, or directions to set up Postgres permissions. I also create slides with markdown. I\u0026rsquo;ll only note something if it\u0026rsquo;s easier to reference my notes than search for it online. I also take notes to help students I mentor.\nI use a combination of Apple Reminders and private GitHub issues with task lists for short term personal tasks, and GitHub Projects to visualize Kanban project and portfolio management. But for notes, I like good ol\u0026rsquo; fashioned git and markdown. I use a private repository for my personal notes, and a separate repo for my public notes. I believe this method is the strongest embrace of trustworthy technology. Git and markdown are two timeless tools that are sure to be around for a long time.\nJune 2020 update I began experimenting with a much simpler approach for tracking work-in-progress: todo.txt format. Aug. 2024 update As a parent with limited time, I\u0026rsquo;ve simplified to using my phone\u0026rsquo;s built-in notes app for quick capture. Sometimes convenience trumps optimal workflow and the stage of life I\u0026rsquo;m in calls for it. ","date_published":"2017-02-06T00:00:00Z","date_modified":"2017-02-06T00:00:00Z"},{"id":"3c66b97d-3906-5ff5-f959-15ca8ef956f1","url":"https://brett.cloud/about/","title":"Brett","summary":" code with me recruit me work with me Brett Gardiner is passionate at writing enterprise software fit for solving problems at different scales and velocities. Recently, Brett has been hard at work optimizing realtime communication with hundreds of thousands of devices. ","content_text":" code with me recruit me work with me Brett Gardiner is passionate at writing enterprise software fit for solving problems at different scales and velocities. Recently, Brett has been hard at work optimizing realtime communication with hundreds of thousands of devices. GitHub LinkedIn What do I like? I like my family, pizza, Linux, reading, spinach but not arugula, early morning hikes in the Rocky Mountains, custom keyboards, movies, and tinkering with IoT devices.\nWhat are my hobbies? I play pickleball, hike and camp. I also have numerous ongoing projects such as a self-hosted kubernetes homelab and moonlighting programming projects.\nHow do we get in touch? Please try one of the channels linked above.\nYou may also reach me at:\nhi@[this website's domain]","banner_image":"https://brett.cloud/about/desk_hu_8026f734d93370aa.png","date_published":"0001-01-01T00:00:00Z","date_modified":"0001-01-01T00:00:00Z"}]}